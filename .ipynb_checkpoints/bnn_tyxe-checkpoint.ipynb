{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c130a0-f6bf-46fa-85ca-6a893b07b90b",
   "metadata": {},
   "source": [
    "# TyXe \n",
    "\n",
    "TyXe (Ancient greek: goddess of chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952c20d5-5238-4e6d-954f-e90170929bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e49bac-1288-4863-a3f6-2798f6b8df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first some imports:\n",
    "\n",
    "import contextlib\n",
    "import functools\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import tyxe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57eb3231-a267-4bf0-b8c0-7c5fb99bf5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset helper functions\n",
    "\n",
    "from utils import make_loaders_resnet_cifar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590c455a-c2af-45ff-adbf-ff42f0f89e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- PARAMETERS OF BAYESIAN RESNET TRAINING ------\n",
    "\n",
    "inference: str = \"ml\"\n",
    "architecture: str = \"resnet18\"\n",
    "dataset: str = \"cifar10\" # 10 or 100\n",
    "train_batch_size: int = 10\n",
    "test_batch_size: int = 10\n",
    "local_reparameterization: bool = False # important: variance reduction for gradients!\n",
    "flipout: bool = False\n",
    "num_epochs: int = 1\n",
    "test_samples: int = 20\n",
    "max_guide_scale: float = 0.1 # to prevent underfitting\n",
    "rank: int = 10\n",
    "root: str = os.environ.get(\"DATSETS_PATH\", \"./data\")\n",
    "seed: int = 42\n",
    "output_dir: Optional[str] = None\n",
    "pretrained_weights: Optional[str] = None # path to pretrained weights\n",
    "scale_only: bool = False\n",
    "lr: float = 0.001\n",
    "milestones: Optional[List[int]] = None\n",
    "gamma: float = 0.1\n",
    "mock_dataset: bool = False\n",
    "\n",
    "# ----- check args: inference, architecture, dataset ------\n",
    "inference_options = [\n",
    "    \"ml\",\n",
    "    \"map\",\n",
    "    \"mean-field\",\n",
    "    \"last-layer-mean-field\",\n",
    "    \"last-layer-full\",\n",
    "    \"last-layer-low-rank\"\n",
    "]\n",
    "assert inference in inference_options, inference\n",
    "\n",
    "resnets = [n for n in dir(torchvision.models) if (n.startswith(\"resnet\") or n.startswith(\"wide_resnet\")) and n[-1].isdigit()]\n",
    "assert architecture in resnets, architecture\n",
    "\n",
    "datasets = [\"cifar10\", \"cifar100\", \"mnist\"]\n",
    "assert dataset in datasets, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b9251-3203-48d3-bdc4-fcea99d1dabc",
   "metadata": {},
   "source": [
    "### Initialize our Dataset and Model\n",
    "* arbitrary pytorch datasets and models work\n",
    "* it is straightforward to go integrate TyXe into any existing Pytorch workflow since we just start out with an arbitrary `torch.nn.Module`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49643f8-6565-40ee-82c9-4ff725ea25f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "# ----- set up pyro & torch -----\n",
    "pyro.set_rng_seed(seed)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# ----- set up dataset and model -----\n",
    "def make_net(dataset, architecture):\n",
    "    net = getattr(torchvision.models, architecture)(pretrained=True)\n",
    "    if dataset.startswith(\"cifar\"):\n",
    "        net.conv1 = nn.Conv2d(3, net.conv1.out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        net.maxpool = nn.Identity()\n",
    "        num_classes = 10 if dataset.endswith(\"10\") else 100\n",
    "        net.fc = nn.Linear(net.fc.in_features, num_classes)\n",
    "    return net\n",
    "\n",
    "train_loader, test_loader, ood_loader = make_loaders(dataset, root, train_batch_size, test_batch_size, use_cuda, mock_dataset)\n",
    "net: torch.nn.Module = make_net(dataset, architecture).to(device)\n",
    "if pretrained_weights is not None:\n",
    "    sd = torch.load(pretrained_weights, map_location=device)\n",
    "    net.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b50830-8337-41ce-b004-50b52d2f6e52",
   "metadata": {},
   "source": [
    "### Set up ResNet to be Bayesian using TyXe\n",
    "\n",
    "To set up a Bayesian Model in TyXe we need to select one of a few options for each of:\n",
    "\n",
    "1. **Likelihood** - The Likelihood of our Training Data $p(D|\\theta)$.\n",
    "2. **Guide** - The Variational Distribution $q(\\theta|D)$. \n",
    "3. **Prior** - Our Prior Belief $p(\\theta)$ about our parameters. Distribution will be around the pretrained weights' values.\n",
    "\n",
    "Our choice for each of these three components does two things:\n",
    "* Determine the family of distributions the particular object may belong to.\n",
    "* Initialize the distribution's parameters.\n",
    "\n",
    "Let's go through the meaning and options for each of the three components, one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb5129-a583-4540-bb03-888d60ba2654",
   "metadata": {},
   "source": [
    "#### 1. Setting up the Likelihood\n",
    "\n",
    "The Likelihood of our Training Data $p(D|\\theta)$. The support of the distribution must be equal in size to the number of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd8e8a-675d-43c4-b271-e95d7d18a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli, Categorical, HeteroskedasticGaussian, HomoskedasticGaussian\n",
    "likelihood = tyxe.likelihoods.Categorical(len(train_loader.sampler))\n",
    "\n",
    "# uncomment for documentation:\n",
    "# tyxe.likelihoods.HeteroskedasticGaussian?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e13e86-1c4a-407a-88a3-cd99e3855779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f314c4-6d20-4fec-9130-7c46c813f4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e68c8-c17e-494a-bbd1-c577e4e4e1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a73c22c-2a32-4632-83ee-0bb15ac20092",
   "metadata": {},
   "source": [
    "#### 2. Setting up our Guide\n",
    "\n",
    "The choice of the variational distribution is where most of our flexibility lies. \n",
    "It is recommended and most convenient to use one of the `Autoguide`s, either from pyro or TyXe.\n",
    "The TyXe `BNN`s expect an only partially initialized `guide` object\n",
    "Let's go through some of the options:\n",
    "1. Maximum Likelihood: it is straightforward to just do maximum likelihood by letting the guide be `None`.\n",
    "2. Maximum a posteriori inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae64345-f124-4b47-a5fb-86d4fff06e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if inference == \"ml\":\n",
    "    # do maximum likelihood\n",
    "    test_samples = 1\n",
    "    guide = None\n",
    "elif inference == \"map\":\n",
    "    # maximum a posteriori inference \n",
    "    test_samples = 1\n",
    "    guide = functools.partial(\n",
    "        pyro.infer.autoguide.AutoDelta,\n",
    "        init_loc_fn=tyxe.guides.PretrainedInitializer.from_net(net)\n",
    "    )\n",
    "elif inference == \"mean-field\":\n",
    "    guide = functools.partial(\n",
    "        tyxe.guides.AutoNormal,\n",
    "        init_loc_fn=tyxe.guides.PretrainedInitializer.from_net(net),\n",
    "        init_scale=1e-4,\n",
    "        max_guide_scale=max_guide_scale, # prevent underfitting\n",
    "        train_loc=not scale_only # train mean parameter?\n",
    "    )\n",
    "elif inference.startswith(\"last-layer\"):\n",
    "    if pretrained_weights is None:\n",
    "        raise ValueError(\"Asked to do last-layer inference, but no pre-trained weights were provided.\")\n",
    "    # turning parameters except for last layer in buffers to avoid training them\n",
    "    # this might be avoidable via poutine.block\n",
    "    for module in net.modules():\n",
    "        if module is not net.fc:\n",
    "            for param_name, param in list(module.named_parameters(recurse=False)):\n",
    "                delattr(module, param_name)\n",
    "                module.register_buffer(param_name, param.detach().data)\n",
    "\n",
    "    if inference == \"last-layer-mean-field\":\n",
    "        guide = functools.partial(\n",
    "            tyxe.guides.AutoNormal, \n",
    "            init_loc_fn=tyxe.guides.PretrainedInitializer.from_net(net),\n",
    "            init_scale=1e-4\n",
    "        )\n",
    "    elif inference == \"last-layer-full\":\n",
    "        guide = functools.partial(\n",
    "            pyro.infer.autoguide.AutoMultivariateNormal,\n",
    "            init_loc_fn=tyxe.guides.PretrainedInitializer.from_net(net),\n",
    "            init_scale=1e-4\n",
    "        )\n",
    "    elif inference == \"last-layer-low-rank\":\n",
    "        guide = functools.partial(\n",
    "            pyro.infer.autoguide.AutoLowRankMultivariateNormal,\n",
    "            rank=rank,\n",
    "            init_loc_fn=tyxe.guides.PretrainedInitializer.from_net(net),\n",
    "            init_scale=1e-4\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(f\"Invalid option for inference: '{inference}''\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Invalid option for inference: '{inference}''\")\n",
    "\n",
    "print(\"Automatic Guide Families:\")\n",
    "print([g for g in dir(pyro.infer.autoguide) if g.startswith(\"Auto\")])\n",
    "\n",
    "# uncomment for documentation:\n",
    "# pyro.infer.autoguide.AutoDelta?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a09124b-3a23-4914-a79e-e968485a0796",
   "metadata": {},
   "source": [
    "#### 3. Setting up our Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423d1d9-124f-4869-842c-1bb3598e7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is standard practice to not be bayesian about batchnorm modules:\n",
    "prior_kwargs = {\n",
    "    \"expose_all\": False, # do not treat all nn.Modules with pyro \n",
    "    \"hide_module_types\": (nn.BatchNorm2d,) # specifically, ignore batchnorms\n",
    "}\n",
    "\n",
    "# our choice of guide impacts how we need to initialize the Prior:\n",
    "if inference == \"ml\":\n",
    "    # do not be bayesian if we are doing maximum likelihood\n",
    "    prior_kwargs[\"hide_all\"] = True\n",
    "elif inference.startswith(\"last-layer\"):\n",
    "    # only be bayesian about the final, fully connected layer\n",
    "    del prior_kwargs['hide_module_types']\n",
    "    prior_kwargs[\"expose_modules\"] = [net.fc]\n",
    "    \n",
    "prior = tyxe.priors.IIDPrior(\n",
    "    dist.Normal(\n",
    "        torch.zeros(1, device=device),\n",
    "        torch.ones(1, device=device)\n",
    "    ),\n",
    "    **prior_kwargs\n",
    ")\n",
    "\n",
    "# IIDPrior, DictPrior, LambdaPrior, LayerwiseNormalPrior\n",
    "print(\"Available Prior Distributions:\")\n",
    "print([p for p in dir(tyxe.priors) if p[0].upper() == p[0] and not \"_\" in p])\n",
    "\n",
    "# uncomment for documentation:\n",
    "# tyxe.priors.IIDPrior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0558721-fd57-4d79-a8e5-6803da65a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally set up our VariationalBNN!\n",
    "bnn = tyxe.VariationalBNN(\n",
    "    net, prior, likelihood, guide\n",
    ")\n",
    "\n",
    "# uncomment for documentation:\n",
    "# bnn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8164242-7504-4e92-aa72-a25c469074f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient variance reduction techniques:\n",
    "if local_reparameterization:\n",
    "    if flipout:\n",
    "        raise RuntimeError(\"Can't use both local reparameterization and flipout, pick one.\")\n",
    "    train_context = tyxe.poutine.local_reparameterization\n",
    "elif flipout:\n",
    "    train_context = tyxe.poutine.flipout\n",
    "else:\n",
    "    train_context = contextlib.nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17ce83dd-9253-4982-8ec5-13491f6f8168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASGD', 'Adadelta', 'Adagrad', 'AdagradRMSProp', 'Adam', 'AdamW', 'Adamax', 'ChainedScheduler', 'ClippedAdam', 'ConstantLR', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts', 'CyclicLR', 'DCTAdam', 'ExponentialLR', 'LambdaLR', 'LinearLR', 'MultiStepLR', 'MultiplicativeLR', 'NAdam', 'OneCycleLR', 'PyroLRScheduler', 'PyroOptim', 'RAdam', 'RMSprop', 'ReduceLROnPlateau', 'Rprop', 'SGD', 'SequentialLR', 'SparseAdam', 'StepLR']\n"
     ]
    }
   ],
   "source": [
    "# pyro-specific: optimizer must come from pyro.optim\n",
    "if milestones is None:\n",
    "    optim = pyro.optim.Adam({\"lr\": lr})\n",
    "else:\n",
    "    optimizer = torch.optim.Adam\n",
    "    optim = pyro.optim.MultiStepLR({\"optimizer\": optimizer, \"optim_args\": {\"lr\": lr}, \"milestones\": milestones, \"gamma\": gamma})\n",
    "\n",
    "print(\"All typical optimizers & schedulers are supported by pyro.optim:\")\n",
    "print([opt for opt in dir(pyro.optim) if \"_\" not in opt and opt[0] == opt[0].upper()])\n",
    "    \n",
    "# tyXe-specific: evaluation and logging may be done using a callback function, passed to the bnn.fit() method\n",
    "# callback is called after every epoch with the following arguments:\n",
    "def callback(\n",
    "        b: tyxe.VariationalBNN, # bnn\n",
    "        i: int, # epoch number\n",
    "        avg_elbo: float # mean elbo this epoch\n",
    "    ):\n",
    "    avg_err, avg_ll = 0., 0.\n",
    "    for x, y in iter(test_loader):\n",
    "        err, ll = b.evaluate(x.to(device), y.to(device), num_predictions=test_samples)\n",
    "        avg_err += err / len(test_loader.sampler)\n",
    "        avg_ll += ll / len(test_loader.sampler)\n",
    "    print(f\"ELBO={avg_elbo}; test error={100 * avg_err:.2f}%; LL={avg_ll:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11d17e-a4c5-443e-a9a9-c425cab783b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d071f11-1f68-4123-82fa-7bd21b05b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ TRAIN THE MODEL ------\n",
    "with train_context():\n",
    "    bnn.fit(train_loader, optim, num_epochs, callback=callback, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a3c9f-080d-4b57-a871-9713e2c840a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally store results by simply using torch.save:\n",
    "if output_dir is not None:\n",
    "    pyro.get_param_store().save(os.path.join(output_dir, \"param_store.pt\"))\n",
    "    torch.save(bnn.state_dict(), os.path.join(output_dir, \"state_dict.pt\"))\n",
    "\n",
    "    test_predictions = torch.cat([bnn.predict(x.to(device), num_predictions=test_samples)\n",
    "                                  for x, _ in iter(test_loader)])\n",
    "    torch.save(test_predictions.detach().cpu(), os.path.join(output_dir, \"test_predictions.pt\"))\n",
    "\n",
    "    ood_predictions = torch.cat([bnn.predict(x.to(device), num_predictions=test_samples)\n",
    "                                 for x, _ in iter(ood_loader)])\n",
    "    torch.save(ood_predictions.detach().cpu(), os.path.join(output_dir, \"ood_predictions.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681240e-c8b6-4b91-a8ef-50009a03ec2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "bayes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
